{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bca0ecc",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69801d",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "We'll use the MovieLens 100k dataset, which contains 100,000 movie ratings from 943 users on 1,682 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download movie ratings data\n",
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "# Unzip it\n",
    "!unzip ml-100k.zip\n",
    "# Remove the .zip file\n",
    "!rm ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ratings data\n",
    "ratings = pd.read_csv(\n",
    "    \"ml-100k/u.data\",\n",
    "    sep=\"\\t\",  # Tab-separated data\n",
    "    names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"],  # Column names\n",
    "    usecols=[\"user_id\", \"movie_id\", \"rating\"],  # We don't need the timestamp column\n",
    "    low_memory=False\n",
    ")\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860fd858",
   "metadata": {},
   "source": [
    "# Train-Test Split\n",
    "\n",
    "We'll split the data into training and test sets (80/20 split) while ensuring each user has data in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perc = 0.2\n",
    "\n",
    "# Initialize the train and test dataframes\n",
    "train_set, test_set = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# Split data for each user\n",
    "for user_id in ratings.user_id.unique():\n",
    "    # Select only samples of the current user and shuffle\n",
    "    user_df = ratings[ratings.user_id == user_id].sample(\n",
    "        frac=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    n_entries = len(user_df)\n",
    "    n_test = int(round(test_perc * n_entries))\n",
    "    \n",
    "    test_set = pd.concat((test_set, user_df.tail(n_test)))\n",
    "    train_set = pd.concat((train_set, user_df.head(n_entries - n_test)))\n",
    "\n",
    "# Shuffle the final datasets\n",
    "train_set = train_set.sample(frac=1).reset_index(drop=True)\n",
    "test_set = test_set.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf6ec1",
   "metadata": {},
   "source": [
    "# Compute Problem Dimensions\n",
    "\n",
    "Determine the number of unique users and movies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = ratings.user_id.unique().shape[0]\n",
    "n_movies = ratings.movie_id.unique().shape[0]\n",
    "n_users, n_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf2f7f",
   "metadata": {},
   "source": [
    "# Model: Matrix Factorization\n",
    "\n",
    "The `H` class implements the matrix factorization model. It initializes two matrices:\n",
    "- `P`: User latent feature matrix (n_users × r)\n",
    "- `Q`: Movie latent feature matrix (n_movies × r)\n",
    "\n",
    "where `r` is the number of latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647558e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H:\n",
    "    \"\"\"\n",
    "    Matrix Factorization model for collaborative filtering.\n",
    "    \n",
    "    This class represents a recommender system that factorizes the user-item rating\n",
    "    matrix into two lower-dimensional matrices: P (user features) and Q (item features).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of users\n",
    "    m : int\n",
    "        Number of movies/items\n",
    "    r : int\n",
    "        Number of latent factors (rank of the factorization)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    P : numpy.ndarray\n",
    "        User feature matrix of shape (n, r)\n",
    "    Q : numpy.ndarray\n",
    "        Movie feature matrix of shape (m, r)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, m, r):\n",
    "        \"\"\"Initialize the model with random feature matrices.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.P = np.random.rand(n, r)\n",
    "        self.Q = np.random.rand(m, r)\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Generate predicted ratings for all user-movie pairs.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Predicted rating matrix of shape (n_users, n_movies)\n",
    "        \"\"\"\n",
    "        return self.P @ self.Q.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f659b82",
   "metadata": {},
   "source": [
    "# Loss Function: Mean Squared Error\n",
    "\n",
    "The MSE function computes the mean squared error between predicted and actual ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(ratings, R_hat):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error for the given ratings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ratings : pandas.DataFrame\n",
    "        DataFrame containing 'user_id', 'movie_id', and 'rating' columns\n",
    "    R_hat : numpy.ndarray\n",
    "        Predicted rating matrix of shape (n_users, n_movies)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean squared error between predicted and actual ratings\n",
    "    \"\"\"\n",
    "    # Extract predictions for the given user-movie pairs\n",
    "    predictions = R_hat[ratings['user_id'] - 1, ratings['movie_id'] - 1]\n",
    "    \n",
    "    # Get actual ratings\n",
    "    actual = ratings['rating']\n",
    "    \n",
    "    # Return mean of squared differences\n",
    "    return np.mean((predictions - actual) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e79d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and compute initial MSE\n",
    "h = H(n_users, n_movies, 5)\n",
    "mse(train_set, h.predict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a32db",
   "metadata": {},
   "source": [
    "# Training: Gradient Descent\n",
    "\n",
    "We'll train the model using gradient descent to minimize the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf1e215",
   "metadata": {},
   "source": [
    "## Gradient Derivation\n",
    "\n",
    "For the loss function $L = (x^T y - a)^2$, the gradients are:\n",
    "- $\\nabla_x L = 2(x^T y - a) \\cdot y$\n",
    "- $\\nabla_y L = 2(x^T y - a) \\cdot x$\n",
    "\n",
    "We can verify this mathematically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61580743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "n = 3\n",
    "a = sp.Symbol('a')\n",
    "x = sp.Matrix(sp.symbols(f'x0:{n}'))\n",
    "y = sp.Matrix(sp.symbols(f'y0:{n}'))\n",
    "\n",
    "# Define loss function\n",
    "f = (x.dot(y) - a)**2\n",
    "\n",
    "# Compute gradients\n",
    "grad_x = sp.Matrix([sp.diff(f, xi) for xi in x])\n",
    "grad_y = sp.Matrix([sp.diff(f, yi) for yi in y])\n",
    "\n",
    "print(\"∇x f =\")\n",
    "sp.pprint(grad_x)\n",
    "print(\"\\n∇y f =\")\n",
    "sp.pprint(grad_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler verification with scalar variables\n",
    "x, y, a = sp.symbols('x y a', real=True)\n",
    "f = (x * y - a)**2\n",
    "\n",
    "grad_x = sp.diff(f, x)\n",
    "grad_y = sp.diff(f, y)\n",
    "\n",
    "sp.pprint(grad_x)\n",
    "sp.pprint(grad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee222ad9",
   "metadata": {},
   "source": [
    "## Gradient Update Function\n",
    "\n",
    "This function implements one epoch of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_gradient(h, ratings, lr):\n",
    "    \"\"\"\n",
    "    Perform one epoch of stochastic gradient descent to update P and Q.\n",
    "    \n",
    "    For each rating in the dataset, compute the prediction error and update\n",
    "    the corresponding user and movie feature vectors in the direction that\n",
    "    reduces the error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    h : H\n",
    "        The matrix factorization model instance\n",
    "    ratings : pandas.DataFrame\n",
    "        DataFrame containing 'user_id', 'movie_id', and 'rating' columns\n",
    "    lr : float\n",
    "        Learning rate for gradient descent\n",
    "    \"\"\"\n",
    "    for _, user_id, movie_id, rating in ratings.itertuples():\n",
    "        u = user_id - 1  # Convert to 0-indexed\n",
    "        i = movie_id - 1  # Convert to 0-indexed\n",
    "        \n",
    "        # 1. Compute prediction\n",
    "        prediction = h.Q[i] @ h.P[u]\n",
    "        \n",
    "        # 2. Compute error\n",
    "        error = prediction - rating\n",
    "        \n",
    "        # 3. Update feature vectors\n",
    "        # Store current P[u] before updating (needed for Q update)\n",
    "        current_P = h.P[u].copy()\n",
    "        h.P[u] = h.P[u] - lr * (h.Q[i] * error * 2)\n",
    "        h.Q[i] = h.Q[i] - lr * (current_P * error * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bb4fc",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train the model for 100 epochs and track the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59987c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 100\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "mse_history = []\n",
    "for i in range(epochs):\n",
    "    update_gradient(h, train_set, lr)\n",
    "    value = mse(train_set, h.predict())\n",
    "    mse_history.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6246d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.plot(mse_history)\n",
    "plt.title(\"Training Loss (MSE) over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299099f2",
   "metadata": {},
   "source": [
    "# Evaluation on Test Set\n",
    "\n",
    "Now let's evaluate how well our trained model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test set MSE\n",
    "test_mse = mse(test_set, h.predict())\n",
    "print(f\"Test Set MSE: {test_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e2700",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "Let's analyze the distribution of prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93fb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute absolute error for each test rating\n",
    "errors = []\n",
    "for _, user_id, movie_id, rating in test_set.itertuples():\n",
    "    predicted = round(h.predict()[user_id - 1, movie_id - 1])\n",
    "    errors.append(abs(predicted - rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88559dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of errors\n",
    "error_series = pd.Series(errors)\n",
    "error_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error distribution\n",
    "error_series.plot(kind='hist', bins=10)\n",
    "plt.title(\"Distribution of Prediction Errors\")\n",
    "plt.xlabel(\"Absolute Error\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454fe1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of each error value\n",
    "error_series.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefec466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the range of predicted ratings\n",
    "predicted_ratings = np.unique(np.round(h.predict()))\n",
    "print(f\"Unique predicted ratings: {predicted_ratings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2689af7",
   "metadata": {},
   "source": [
    "# Movie Metadata Analysis (Optional)\n",
    "\n",
    "For further exploration, we can load movie metadata to understand which types of movies our model performs best on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d56cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for the movie metadata file\n",
    "columns = [\n",
    "    \"movie_id\", \"title\", \"release_date\", \"video_release_date\", \"IMDb_URL\",\n",
    "    \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\",\n",
    "    \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\", \"Mystery\",\n",
    "    \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
    "]\n",
    "\n",
    "# Load movie metadata\n",
    "df_items = pd.read_csv(\n",
    "    \"ml-100k/u.item\",\n",
    "    sep=\"|\",\n",
    "    names=columns,\n",
    "    encoding=\"ISO-8859-1\"\n",
    ")\n",
    "\n",
    "print(df_items.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188dc85",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a complete implementation of matrix factorization for collaborative filtering from scratch. The model successfully learns latent representations of users and movies that can predict ratings with reasonable accuracy (MSE ≈ 0.91 on the test set).\n",
    "\n",
    "Key takeaways:\n",
    "- Matrix factorization is a powerful technique for recommender systems\n",
    "- Gradient descent can effectively optimize the factorization\n",
    "- The model captures latent patterns in user preferences without explicit content information"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
